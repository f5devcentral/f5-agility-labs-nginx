mode: standalone
adminServer:
  address: :8080
server:
  address: :4141

# The routes determine on what URL path the AIGW is listening
routes:
  - path: /api/chat
    policy: arcadia_ai_policy
    timeoutSeconds: 600
    schema: openai

# What policy is applied to the route
policies:
  - name: arcadia_ai_policy
    profiles:
      - name: default

# To what LLM endpoint we forward the request to
services:
  - name: ollama
    executor: http
    config:
      endpoint: "http://ollama_public_ip:11434/api/chat"
      schema: ollama-chat

# What do we do with the request, at the moment we just forward it
profiles:
  - name: default
    services:
      - name: ollama