mode: standalone
adminServer:
  address: :8080
server:
  address: :4141

# The routes determine on what URL path the AIGW is listening
routes:
  - path: /api/chat
    policy: arcadia_ai_policy
    timeoutSeconds: 600
    schema: openai

# What policy is applied to the route
policies:
  - name: arcadia_ai_policy
    profiles:
      - name: default

# To what LLM endpoint we forward the request to
services:
  - name: ollama
    executor: http
    config:
      endpoint: "http://ollama_public_ip:11434/api/chat"
      schema: ollama-chat

  - name: ollama-mistral
    executor: http
    config:
      endpoint: "http://ollama_public_ip:11434/api/chat"
      schema: ollama-mistral

  - name: ollama-<another_model>
    executor: http
    config:
      endpoint: "http://ollama_public_ip:11434/api/chat"
      schema: ollama-<another_model>

# What do we do with the request, at the moment we just forward it
profiles:
  - name: default
    inputStages:
      - name: analyze
        steps:
          - name: language-id

    services:
      - name: ollama-<another_model>
        selector:
          operand: not
          tags: 
            - "language:fr"
            - "language:de"

      - name: ollama-mistral
        selector:
          tags:
            - "language:fr"

      - name: ollama
        selector:
          tags:
            - "language:de"

# Here we will find all our processor configuration
processors:
  - name: language-id
    type: external
    config:
      endpoint: "http://aigw-processors-f5:8000"
      version: 1
      namespace: f5
    params:
      multi_detect: true